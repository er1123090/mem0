---
title: Configurations
icon: "gear"
iconType: "solid"
---

## How to define configurations?

The `config` is defined as a TypeScript object with two main keys:
- `llm`: Specifies the LLM provider and its configuration
  - `provider`: The name of the LLM (e.g., "openai", "groq")
  - `config`: A nested object containing provider-specific settings

### Config Values Precedence

Config values are applied in the following order of precedence (from highest to lowest):

1. Values explicitly set in the `config` object
2. Environment variables (e.g., `OPENAI_API_KEY`, `OPENAI_API_BASE`)
3. Default values defined in the LLM implementation

This means that values specified in the `config` object will override corresponding environment variables, which in turn override default values.

## How to Use Config

Here's a general example of how to use the config with Mem0:

```typescript
import { Memory } from 'mem0ai/oss';

const config = {
  llm: {
    provider: 'openai',
    config: {
      apiKey: process.env.OPENAI_API_KEY || '',
      model: 'gpt-4-turbo-preview',
      temperature: 0.2,
      maxTokens: 1500,
    },
  },
  embedder: {
    provider: 'openai',
    config: {
      apiKey: process.env.OPENAI_API_KEY || '',
      model: 'text-embedding-3-small',
    },
  },
  vectorStore: {
    provider: 'memory',
    config: {
      collectionName: 'memories',
      dimension: 1536,
    },
  },
  historyDbPath: 'memory.db',
};

const memory = new Memory(config);
memory.add("Your text here", { userId: "user123", metadata: { category: "example" } });
```

## Why is Config Needed?

Config is essential for:
1. Specifying which LLM to use.
2. Providing necessary connection details (e.g., model, api_key, temperature).
3. Ensuring proper initialization and connection to your chosen LLM.

## Master List of All Params in Config

Here's a comprehensive list of all parameters that can be used across different LLMs:

| Parameter            | Description                                   | Provider          |
|----------------------|-----------------------------------------------|-------------------|
| `model`              | Embedding model to use                        | All               |
| `temperature`        | Temperature of the model                      | All               |
| `apiKey`            | API key to use                                | All               |
| `maxTokens`         | Tokens to generate                            | All               |
| `topP`              | Probability threshold for nucleus sampling    | All               |
| `topK`              | Number of highest probability tokens to keep  | All               |
| `openaiBaseUrl`    | Base URL for OpenAI API                       | OpenAI            |

## Supported LLMs

For detailed information on configuring specific LLMs, please visit the [LLMs](./models) section. There you'll find information for each supported LLM with provider-specific usage examples and configuration details.
